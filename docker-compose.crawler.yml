# docker-compose.crawler.yml
# 本地爬虫节点 - 通过 Tailscale 连接远程 Redis
#
# 使用方法:
#   1. 确保本地已加入 Tailscale 且能 ping 通 EC2
#   2. 创建 .env.crawler 文件 (参考 .env.crawler.example)
#   3. docker compose -f docker-compose.crawler.yml up -d --build
#   4. 启用监控: docker compose -f docker-compose.crawler.yml --profile monitoring up -d --build

services:
  # Python Crawler (主要)
  py-crawler:
    build:
      context: .
      dockerfile: build/Dockerfile.py-crawler
    container_name: animehot-py-crawler-local
    restart: always
    stop_grace_period: 30s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    environment:
      # Redis 连接 (Tailscale 内网，兼容 Go 爬虫的 REDIS_REMOTE_ADDR)
      REDIS_REMOTE_ADDR: ${REDIS_REMOTE_ADDR:-100.99.127.100:6379}
      REDIS_PASSWORD: ${REDIS_PASSWORD:-}

      # 限流 (与 Go 爬虫共享配置)
      APP_RATE_LIMIT: ${APP_RATE_LIMIT:-2}
      APP_RATE_BURST: ${APP_RATE_BURST:-5}

      # 爬虫配置
      CRAWLER_MAX_CONCURRENT_TASKS: ${CRAWLER_MAX_CONCURRENT_TASKS:-3}

      # 端口
      HEALTH_PORT: "8081"
      METRICS_PORT: "2112"

      # 日志
      LOG_LEVEL: ${APP_LOG_LEVEL:-info}
      TZ: Asia/Tokyo

    # 使用 host 网络模式以访问 Tailscale
    network_mode: host

    # 内存限制
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 512M

  # Go Crawler (已废弃，保留以便回滚)
  crawler:
    build:
      context: .
      dockerfile: build/Dockerfile.crawler
    container_name: animehot-crawler-local
    profiles: ["go-crawler"]  # 需要显式启用: --profile go-crawler
    restart: always
    init: true
    stop_grace_period: 2m
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    environment:
      APP_ENV: prod
      DEBUG: "false"
      TZ: Asia/Tokyo
      REDIS_ADDR: ${REDIS_REMOTE_ADDR:-100.99.127.100:6379}
      REDIS_PASSWORD: ""
      BROWSER_MAX_CONCURRENCY: ${BROWSER_MAX_CONCURRENCY:-2}
      BROWSER_MAX_FETCH_COUNT: ${BROWSER_MAX_FETCH_COUNT:-120}
      BROWSER_PAGE_TIMEOUT: ${BROWSER_PAGE_TIMEOUT:-60s}
      BROWSER_TASK_TIMEOUT: ${BROWSER_TASK_TIMEOUT:-12m}
      MAX_TASKS: ${MAX_TASKS:-50}
      SCHEDULER_PAGES_ON_SALE: ${SCHEDULER_PAGES_ON_SALE:-5}
      SCHEDULER_PAGES_SOLD: ${SCHEDULER_PAGES_SOLD:-5}
      APP_RATE_LIMIT: ${APP_RATE_LIMIT:-3}
      APP_RATE_BURST: ${APP_RATE_BURST:-5}
      HTTP_PROXY: ${HTTP_PROXY:-}
      PROXY_AUTO_SWITCH: "false"
      APP_METRICS_ADDR: ":2112"
      APP_LOG_LEVEL: ${APP_LOG_LEVEL:-info}
    network_mode: host
    cap_add:
      - SYS_ADMIN
    security_opt:
      - seccomp=unconfined
    shm_size: '2gb'
    tmpfs:
      - /tmp:mode=1777,exec

  # =============================================================================
  # Monitoring - Grafana Cloud (可选)
  # =============================================================================

  alloy:
    image: grafana/alloy:latest
    container_name: animehot-alloy-local
    restart: unless-stopped
    profiles: ["monitoring"]
    command:
      - run
      - /etc/alloy/config.river
    environment:
      TZ: Asia/Tokyo
      GRAFANA_CLOUD_PROM_REMOTE_WRITE_URL: ${GRAFANA_CLOUD_PROM_REMOTE_WRITE_URL}
      GRAFANA_CLOUD_PROM_USERNAME: ${GRAFANA_CLOUD_PROM_USERNAME}
      GRAFANA_CLOUD_PROM_API_KEY: ${GRAFANA_CLOUD_PROM_API_KEY}
      GRAFANA_CLOUD_LOKI_URL: ${GRAFANA_CLOUD_LOKI_URL}
      GRAFANA_CLOUD_LOKI_USERNAME: ${GRAFANA_CLOUD_LOKI_USERNAME}
      GRAFANA_CLOUD_LOKI_API_KEY: ${GRAFANA_CLOUD_LOKI_API_KEY}
      HOSTNAME: ${HOSTNAME:-animehot-local}
    volumes:
      - ./deploy/alloy/config.crawler.river:/etc/alloy/config.river:ro
      - /var/run/docker.sock:/var/run/docker.sock
    # 使用 host 网络以抓取 crawler 的 metrics (localhost:2112)
    network_mode: host
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

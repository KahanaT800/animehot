# docker-compose.crawler.yml
# 本地爬虫节点 - 通过 Tailscale 连接远程 Redis
#
# 使用方法:
#   1. 确保本地已加入 Tailscale 且能 ping 通 EC2
#   2. 创建 .env.crawler 文件 (参考 .env.crawler.example)
#   3. docker compose -f docker-compose.crawler.yml up -d
#   4. 启用监控: docker compose -f docker-compose.crawler.yml --profile monitoring up -d

services:
  crawler:
    build:
      context: .
      dockerfile: build/Dockerfile.crawler
    container_name: animehot-crawler-local
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    environment:
      # App
      APP_ENV: prod
      DEBUG: "false"
      TZ: Asia/Tokyo

      # Redis 连接 (Tailscale 内网)
      REDIS_ADDR: ${REDIS_REMOTE_ADDR:-100.99.127.100:6379}
      REDIS_PASSWORD: ""

      # 爬虫配置
      BROWSER_MAX_CONCURRENCY: ${BROWSER_MAX_CONCURRENCY:-2}
      BROWSER_MAX_FETCH_COUNT: ${BROWSER_MAX_FETCH_COUNT:-120}
      BROWSER_PAGE_TIMEOUT: ${BROWSER_PAGE_TIMEOUT:-60s}
      BROWSER_TASK_TIMEOUT: ${BROWSER_TASK_TIMEOUT:-12m}
      MAX_TASKS: ${MAX_TASKS:-50}

      # 调度配置
      SCHEDULER_PAGES_ON_SALE: ${SCHEDULER_PAGES_ON_SALE:-5}
      SCHEDULER_PAGES_SOLD: ${SCHEDULER_PAGES_SOLD:-5}

      # 限流
      APP_RATE_LIMIT: ${APP_RATE_LIMIT:-3}
      APP_RATE_BURST: ${APP_RATE_BURST:-5}

      # 代理 (可选)
      HTTP_PROXY: ${HTTP_PROXY:-}
      PROXY_AUTO_SWITCH: "false"

      # Metrics
      APP_METRICS_ADDR: ":2112"

      # 日志
      APP_LOG_LEVEL: ${APP_LOG_LEVEL:-info}

    # 使用 host 网络模式以访问 Tailscale
    network_mode: host

    # Chrome 需要的权限
    cap_add:
      - SYS_ADMIN
    security_opt:
      - seccomp=unconfined

    # 共享内存 (Chrome 需要)
    shm_size: '2gb'

    tmpfs:
      - /tmp:mode=1777,exec

  # =============================================================================
  # Monitoring - Grafana Cloud (可选)
  # =============================================================================

  alloy:
    image: grafana/alloy:latest
    container_name: animehot-alloy-local
    restart: unless-stopped
    profiles: ["monitoring"]
    command:
      - run
      - /etc/alloy/config.river
    environment:
      TZ: Asia/Tokyo
      GRAFANA_CLOUD_PROM_REMOTE_WRITE_URL: ${GRAFANA_CLOUD_PROM_REMOTE_WRITE_URL}
      GRAFANA_CLOUD_PROM_USERNAME: ${GRAFANA_CLOUD_PROM_USERNAME}
      GRAFANA_CLOUD_PROM_API_KEY: ${GRAFANA_CLOUD_PROM_API_KEY}
      GRAFANA_CLOUD_LOKI_URL: ${GRAFANA_CLOUD_LOKI_URL}
      GRAFANA_CLOUD_LOKI_USERNAME: ${GRAFANA_CLOUD_LOKI_USERNAME}
      GRAFANA_CLOUD_LOKI_API_KEY: ${GRAFANA_CLOUD_LOKI_API_KEY}
      HOSTNAME: ${HOSTNAME:-animehot-local}
    volumes:
      - ./deploy/alloy/config.crawler.river:/etc/alloy/config.river:ro
      - /var/run/docker.sock:/var/run/docker.sock
    # 使用 host 网络以抓取 crawler 的 metrics (localhost:2112)
    network_mode: host
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
